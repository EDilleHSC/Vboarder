# ğŸ§  Reasoning Kernel Quick Start

## What You Just Built

âœ… **Multi-step reasoning system** for VBoarder agents
âœ… **Confidence-based iteration** (loops until high confidence)
âœ… **Dynamic model routing** (task complexity â†’ model slot)
âœ… **Heuristic scorer** (ready for ML upgrade later)
âœ… **New `/ask` endpoint** with full agent integration

---

## ğŸ“ New Files Created

```text
reasoning_kernel.py          # Core iteration loop engine
router.py                    # Task-based model slot selection
scorer_stub.py               # Confidence heuristics
tests_flat/eval_reasoning.py # Evaluation suite
docs/REASONING_KERNEL.md     # Full documentation
.env.reasoning               # Feature flag config
```

---

## ğŸš€ Test It Now

### 1. Start Backend (if not running)

**WSL Terminal:**
```bash
cd /mnt/d/ai/projects/vboarder
source .venv-wsl/bin/activate
bash start_vboarder.sh
```

### 2. Quick Smoke Test

```bash
curl -X POST "http://127.0.0.1:3738/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "task": "Say hello in 5 words or less",
    "agent_role": "CEO",
    "max_iterations": 2
  }'
```

**Expected Response:**
```json
{
  "status": "success",
  "agent": "CEO",
  "result": "Hello, how can I help?",
  "iterations": 1,
  "confidence": 0.95,
  "reasoning_status": "success",
  "routing": {
    "slot": "slot:a",
    "model": "mistral:latest",
    "complexity": "simple"
  }
}
```

### 3. Run Full Evaluation

```bash
python tests_flat/eval_reasoning.py
```

This tests:
- âœ… COO task scheduling
- âœ… CTO risk assessment
- âœ… CEO strategic planning
- âœ… CFO budget analysis

**Output saved to:** `out/reasoning_eval.json`

---

## ğŸ¯ How It Works

```text
User â†’ /ask â†’ Router â†’ Reasoning Kernel
                â†“
        1. Pick model slot (a/b/c)
        2. Generate response
        3. Score confidence
        4. High enough? â†’ Done
        5. Too low? â†’ Refine prompt, loop
        6. Max iterations? â†’ Return best attempt
```

### Confidence Scoring

**Heuristics (scorer_stub.py):**
- âœ… Length (longer = more detail)
- âœ… Structure (numbered lists, bullets)
- âŒ Uncertainty ("maybe", "unclear")
- âŒ Errors ("failed", "broken")
- âœ… Completion markers ("done", "therefore")

**Score ranges:**
- `0.85+` : High confidence (early exit)
- `0.50-0.84` : Medium (refine and retry)
- `<0.50` : Low (escalate for review)

### Model Routing

**Task â†’ Slot mapping:**
- `slot:a` : < 1200 chars, simple, no tools â†’ Fast
- `slot:b` : < 8000 chars, moderate â†’ Balanced
- `slot:c` : Complex, tools needed â†’ Powerful

**Override with env vars:**
```bash
MODEL_SLOT_A=phi:latest         # Tiny fast model
MODEL_SLOT_B=mistral:latest     # Default
MODEL_SLOT_C=llama3:latest      # Larger for complex
```

---

## ğŸ§ª Test Cases

### Simple Query (slot:a)
```bash
curl -X POST http://127.0.0.1:3738/ask -H "Content-Type: application/json" \
  -d '{"task": "What is 2+2?", "agent_role": "CEO"}'
```

### Moderate Task (slot:b)
```bash
curl -X POST http://127.0.0.1:3738/ask -H "Content-Type: application/json" \
  -d '{
    "task": "Schedule weekly ops sync and ensure Q4 deadline compliance",
    "agent_role": "COO",
    "max_iterations": 4
  }'
```

### Complex with Tools (slot:c)
```bash
curl -X POST http://127.0.0.1:3738/ask -H "Content-Type: application/json" \
  -d '{
    "task": "Search latest AI research and summarize top 3 papers",
    "agent_role": "AIR",
    "max_iterations": 5
  }'
```

---

## ğŸ“Š Expected Benchmarks

| Task Type | Iterations | Time | Confidence |
| --------- | ---------- | ---- | ---------- |
| Simple    | 1-2        | <1s  | 0.90+      |
| Moderate  | 2-4        | 2-3s | 0.85+      |
| Complex   | 3-5        | 4-6s | 0.80+      |

**On local Ollama with mistral:latest**

---

## ğŸ”§ Next Steps

### Phase 1: Current MVP âœ…
- [x] Heuristic scorer
- [x] Basic routing
- [x] Iteration loop
- [x] `/ask` endpoint
- [x] Evaluation suite

### Phase 2: ML Scorer (Next 2 weeks)
- [ ] Replace scorer_stub with tiny BERT classifier
- [ ] Train on VBoarder task samples
- [ ] Tune confidence thresholds

### Phase 3: Advanced Models (Next month)
- [ ] Integrate LiquidAI models
- [ ] Test LLM2e (efficient 7B)
- [ ] Benchmark slot performance

### Phase 4: Verification (Future)
- [ ] Multi-agent cross-checking
- [ ] Self-correction loops
- [ ] Fact verification integration

---

## ğŸ› Troubleshooting

### Error: "Reasoning kernel not available"
**Fix:** Check files exist
```bash
ls -l reasoning_kernel.py router.py scorer_stub.py
```

### All responses have low confidence
**Fix:** Adjust heuristics in `scorer_stub.py` or lower threshold
```json
{
  "confidence_threshold": 0.70
}
```

### Always hits max_iterations
**Fix:** Increase max or lower threshold
```json
{
  "max_iterations": 10,
  "confidence_threshold": 0.75
}
```

---

## ğŸ“š Full Documentation

**See:** `docs/REASONING_KERNEL.md`

Includes:
- API reference
- Module details
- Configuration options
- Performance benchmarks
- Roadmap

---

## ğŸ‰ Ready to Commit!

**Branch:** Create `feature/reasoning-kernel`

```bash
git checkout -b feature/reasoning-kernel
git add reasoning_kernel.py router.py scorer_stub.py
git add tests_flat/eval_reasoning.py
git add docs/REASONING_KERNEL.md
git add .env.reasoning
git add api/main.py  # Updated with /ask endpoint
git commit -m "feat: Add reasoning kernel with confidence scoring and dynamic routing"
git push origin feature/reasoning-kernel
```

**Then merge to `dev-temp` and test:**
```bash
git checkout dev-temp
git merge feature/reasoning-kernel
python tests_flat/eval_reasoning.py
```

---

**ğŸš€ VBoarder is now the smartest small-agent framework!**
