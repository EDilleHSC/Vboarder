#!/usr/bin/env python3
"""
SmartHybrid Inference Router v3.2
Includes local Ollama fallback (OpenAI â†’ Gemini â†’ HuggingFace â†’ Ollama)
"""

import os, time, json, logging, httpx, requests
from dotenv import load_dotenv
from datetime import datetime
from typing import Optional, Dict, Any

# === Load environment ===
load_dotenv(override=True)
OPENAI_KEY = os.getenv("OPENAI_API_KEY")
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
HF_TOKEN   = os.getenv("HF_TOKEN")
HF_ENDPOINT = os.getenv("HF_MODEL_ENDPOINT", "https://api-inference.huggingface.co/models/google/gemma-2b")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-3.5-turbo")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://localhost:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "llama3:8b")

# === Logging setup ===
logging.basicConfig(level=logging.INFO, format="%(message)s")

def color(text, c):
    colors = {"green":"\033[92m","yellow":"\033[93m","red":"\033[91m","blue":"\033[94m","reset":"\033[0m"}
    return f"{colors.get(c,'')}{text}{colors['reset']}"

def translate_error(code):
    mapping = {
        401: "Unauthorized â€“ check API key",
        403: "Forbidden â€“ model not allowed",
        404: "Model or endpoint not found",
        429: "Rate limit or quota exceeded",
        500: "Server error â€“ provider issue"
    }
    return mapping.get(code, f"Unexpected error ({code})")

# =====================================================================
# === Provider Queries ===
# =====================================================================

def query_openai(prompt: str) -> Optional[str]:
    """Query OpenAI API"""
    try:
        import openai
        client = openai.OpenAI(api_key=OPENAI_KEY)
        t0 = time.time()
        resp = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[{"role":"user","content":prompt}],
            temperature=0.7,
            max_tokens=4096
        )
        latency = round(time.time()-t0, 2)
        result = resp.choices[0].message.content.strip()
        logging.info(color(f"âœ“ OpenAI success ({latency}s)", "green"))
        return result
    except Exception as e:
        msg = str(e)
        if "429" in msg: msg = "Rate limit or quota exceeded"
        logging.error(color(f"OpenAI Error: {msg}", "red"))
        return None

def query_gemini(prompt: str) -> Optional[str]:
    """Query Gemini"""
    if not GEMINI_KEY:
        logging.error(color("Gemini key missing", "red"))
        return None
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_KEY}"
    body = {"contents":[{"parts":[{"text":prompt}]}]}
    try:
        t0 = time.time()
        r = httpx.post(url, json=body, timeout=20)
        r.raise_for_status()
        latency = round(time.time()-t0, 2)
        data = r.json()
        text = data["candidates"][0]["content"]["parts"][0]["text"]
        logging.info(color(f"âœ“ Gemini success ({latency}s)", "green"))
        return text.strip()
    except httpx.HTTPStatusError as e:
        logging.error(color(f"Gemini HTTP Error: {translate_error(e.response.status_code)}", "red"))
        return None
    except Exception as e:
        logging.error(color(f"Gemini Error: {e}", "red"))
        return None

def query_huggingface(prompt: str) -> Optional[str]:
    """Query Hugging Face"""
    if not HF_TOKEN:
        logging.error(color("HF token missing", "red"))
        return None
    headers = {"Authorization": f"Bearer {HF_TOKEN}", "Content-Type": "application/json"}
    try:
        t0 = time.time()
        r = httpx.post(HF_ENDPOINT, headers=headers, json={"inputs":prompt}, timeout=20)
        r.raise_for_status()
        latency = round(time.time()-t0, 2)
        data = r.json()
        if isinstance(data, list) and data:
            text = data[0].get("generated_text", "")
        else:
            text = data.get("generated_text", "")
        if text:
            logging.info(color(f"âœ“ HuggingFace success ({latency}s)", "green"))
            return text.strip()
        return None
    except httpx.HTTPStatusError as e:
        logging.error(color(f"HuggingFace HTTP Error: {translate_error(e.response.status_code)}", "red"))
        return None
    except Exception as e:
        logging.error(color(f"HuggingFace Error: {e}", "red"))
        return None

def query_ollama(prompt: str) -> Optional[str]:
    """Query local Ollama model"""
    try:
        t0 = time.time()
        resp = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json={"model": OLLAMA_MODEL, "prompt": prompt},
            timeout=45,
        )
        if resp.status_code == 200:
            try:
                data = resp.json()
                text = data.get("response") or str(data)
            except Exception:
                text = resp.text
            latency = round(time.time()-t0, 2)
            logging.info(color(f"âœ“ Ollama success ({latency}s)", "green"))
            return text.strip()
        else:
            logging.error(color(f"Ollama HTTP Error {resp.status_code}", "red"))
            return None
    except Exception as e:
        logging.error(color(f"Ollama Error: {e}", "red"))
        return None

# =====================================================================
# === Router Logic ===
# =====================================================================

def route_prompt(prompt: str) -> Dict[str, Any]:
    logging.info(color(f"ğŸ” Routing prompt: {prompt[:40]}...", "blue"))
    for name, func in [
        ("openai", query_openai),
        ("gemini", query_gemini),
        ("huggingface", query_huggingface),
        ("ollama", query_ollama),
    ]:
        result = func(prompt)
        if result:
            return {"provider": name, "response": result}
    return {"provider": None, "response": "âŒ All providers failed"}

# =====================================================================
# === Main Entrypoint ===
# =====================================================================

if __name__ == "__main__":
    print(color("ğŸš€ SmartHybrid Inference Router v3.2", "yellow"))
    print("="*60)
    print("ğŸ” ENV check â†’", {
        "OPENAI_API_KEY": bool(OPENAI_KEY),
        "GEMINI_API_KEY": bool(GEMINI_KEY),
        "HF_TOKEN": bool(HF_TOKEN)
    })
    print("\nğŸ§ª Running Test Prompt...\n")

    output = route_prompt("Briefly explain the role of a reverse proxy.")
    print(color("\nâœ… Router Result Summary:", "green"))
    print(f"Provider: {output['provider']}")
    print(f"Response: {output['response'][:150]}")
